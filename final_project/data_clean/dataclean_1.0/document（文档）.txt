我们主要使用了pandas模块来进行数据清洗。

需要清洗的原始数据集为”dataset1.csv“，清洗后的数据集为”dataset2.csv“

在进行数据清洗之前，我们先使用info()函数来获取数据集的相关信息。可以看到列的数据类型中有object，int64，float64,以及数据集中的总行数7837。

数据清洗第一步，处理重复值。重复值会影响统计结果，所以要除去重复值。经过(df.duplicated()).sum()的运行，我们发现数据集中没有重复的行，故去除重复值这一过程结束。

然后，处理缺失值。缺失值会干扰到统计分析的进行，缺失值的处理方法主要有删除和填补。“last_fixes”列有缺失值，其对应的”fixes_percent“列为0。由于本数据集中数据较多，且数据特点较为分散，无法选择一个合适的值来填补缺失值，所以优先选择将其删除。删除”fixes_percent“列为0的数据后，再经过(df.isnull()).sum()的运行，我们发现每一列的数据都没有缺失，故处理缺失值这一过程结束。

最后，去除异常值。异常值往往会导致统计模型的过拟合，使得模型无法适应新的数据。什么是异常值需要根据实际情况来判断。
本数据集中，有一些数据明显为异常值。可以看到有一些数据的”fixes_percent“超过了100，不符合常理，在此区间的数据为异常值，需要将其删除。

以下为一些想法，可能会在后期实施：
*其次，一些数据的”total_shas”的值较小（<5），具有一定的偶然性，我觉得可以考虑在后期的数据预处理将其删除。
*然后，绘制箱型图，发现在上限Q3+1.5IQR之上依然有很多数据，这些数据也可看做异常值。