DATA COLLECTION
What we have collected is listed as following, with its references.
file_name: Files¡¯ relative path in linux-stable.git
lines: Files¡¯ lines, which does include blanks lines
Authors: the number of persons who wrote these lines of the files, from v3.0 to now
Shas: the number of shas, among the existing lines of files, from v3.0 to now
the_first_time: the earliest time of commits, among the existing lines of files, from v3.0 to now
the_last_time: the latest time of commits, among the existing lines of files, from v3.0 to now
average_date: the average time of commits, among the existing lines of files, from v3.0 to now
total_shas: the total number of commits, from v3.0 to now
last_fixes: the latest shas of commit with ¡°Fixes: SHA (text)" line
fixes_percent: the proportion of commit with ¡°Fixes: SHA (text)" line against the total commit starting with ¡°commit:¡±
In details, lines, authors, shas, the_first_time, the_last_time, average_date were from command ¡°git blame --no-merges v3.0..HEAD FileName¡±. fixes_percent, total_shas, last_fixes were from "git log -p --no-merges --date-order v3.0..HEAD FileName¡±.
Each file is taken as an object which belongs to StableFiles class. 
1)	StableFiles class has methods of blame() to return ¡°git blame¡± results from git
2)	and gitFixCommits() to get ¡°git log¡± results from git 
3)	and count fixes_percent, total_shas and last_fixes. count_author() is used to count the number of persons who wrote these lines.
4)	count_commit() is used to count the number of shas among the existing lines of files.
5)	count_avetime() and count_lines() are used to count the average time and file lins respectively.
6)	Then, run() function could create StableFiles objects, store the whole data sets as a dataframe and write into ¡°results.csv¡±.
We used a random sample of 8000 files in linux-stable, created 8000 objects of StableFiles and got their features. Finally, we got 7838 files, each of which has 9 features (file_name, lines, authors, shas, the_first_time, the_last_time, average_date, total_shas, last_fixes) and 1 response of fixes_percent.
  During data collection, we met many problems and fortunately, we solved them successfully.

  To reduce running time, we wrote into a csv after all the files¡¯ features were collected, instead of writing files¡¯ features one by one. Since we have so large data set, reducing running time and speeding up data collection are critical. After being obtained from linux-stable, features of a file were stored as a dictionary and appended into a row list. The row list was written into ¡°results.csv¡± when the whole files were appended.
  However, writing into a csv, after all the files¡¯ features were collected, runs the risks of data lost. If the 7999th file meets an unexpected error and process finishes with an exit code, previous features of 7998 files will not be saved and be lost. To miss that maddening case, we use try¡­except to catch any Exception outside the for loop of sample file traversal. If process finishes with an exit code, previous features will be stored into ¡°results.csv¡± safely.
  Besides, it is difficult to extract shas, authors, and time from ¡°git blame --no-merges v3.0..HEAD FileName¡±. Inquiry results are not uniform and the column indexes are dynamitic relative to the maximum length of authors and file paths. We find out the solution of String Splitting Twice. Firstly, inquiry results were read as strings line by line; then, we splitd a line with ¡®)¡¯ firstly and discard line contents at the end; after that, we located shas and time according to their length, which are constant; finally, we split the remaining secondly and joined them to get authors.
  But we are confused that there is a little of files or lines in our sample which does not have time information term when using ¡°git blame --no-merges v3.0..HEAD FileName¡±. 
  By the way, we deleted and saved the wrong files and line in ¡°wrong_file¡± list and ¡°Badtime.txt¡±.
